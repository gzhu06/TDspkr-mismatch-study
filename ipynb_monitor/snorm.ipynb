{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Score normalization\n",
    "This paper did a great job in explaining Score normalization: \n",
    "https://www.sciencedirect.com/science/article/pii/S1051200499903603\n",
    "Notice that equation (1) in this paper is Bayes' Theorem (ignoring P(m) and using P(O|m_W) to represent P(O)).\n",
    "About adaptive score normalization: \n",
    "https://www.isca-speech.org/archive/interspeech_2017/matejka17_interspeech.html\n",
    "\n",
    "This code is adapted from: \n",
    "https://github.com/juanmc2005/SpeakerEmbeddingLossComparison/blob/master/reproduce.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "from tqdm import tqdm\n",
    "from kaldi_io import open_or_fd, read_vec_flt\n",
    "from xarray import DataArray\n",
    "from scipy.spatial.distance import cdist\n",
    "from feerci import feerci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper function to load speaker embeddings from ark files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_vec(scpfile):\n",
    "    cohort_embedding = dict()\n",
    "    test_embedding = dict()\n",
    "    \n",
    "    vecs = []\n",
    "    fd = open_or_fd(scpfile)\n",
    "    for line in tqdm(fd):\n",
    "        key, rxfile = line.decode().split('\\t')\n",
    "        vecs = read_vec_flt(rxfile)\n",
    "        \n",
    "        # choose two dict to store the vecs\n",
    "        spkid = key.split('-')[0]\n",
    "        spknum = int(spkid.split('id')[-1])\n",
    "        if spknum <= 10309 and spknum >= 10270:\n",
    "            test_embedding.setdefault(key, []).append(vecs)\n",
    "        else:\n",
    "            cohort_embedding.setdefault(spkid, []).append(vecs)\n",
    "        \n",
    "    return cohort_embedding, test_embedding\n",
    "\n",
    "# Get trial embeddings from VoxCeleb1\n",
    "def get_trial_vec(testEmbedding, trialFile):\n",
    "    # embedding mapping\n",
    "    cache1 = dict()\n",
    "    cache2 = dict()\n",
    "    \n",
    "    # hash to index mapping\n",
    "    index1 = dict()\n",
    "    index2 = dict()\n",
    "\n",
    "    n_file1 = 0\n",
    "    n_file2 = 0\n",
    "    \n",
    "    with open(trialFile, 'r') as tf:\n",
    "        for trialPair in tf:\n",
    "            file1, file2 = trialPair.split(' ')[1], trialPair.split(' ')[2]\n",
    "            key1 = file1.split('/')[0] + '-' + file1.split('/')[1] + \\\n",
    "                    '-' + file1.split('/')[-1].split('.wav')[0]\n",
    "            key2 = file2.split('/')[0] + '-' + file2.split('/')[1] + \\\n",
    "                    '-' + file2.split('/')[-1].split('.wav')[0]\n",
    "            if key1 not in cache1:\n",
    "                cache1[key1] = testEmbedding[key1]\n",
    "                index1[key1] = n_file1\n",
    "                n_file1 += 1\n",
    "                \n",
    "            if key2 not in cache2:\n",
    "                cache2[key2] = testEmbedding[key2]\n",
    "                index2[key2] = n_file2\n",
    "                n_file2 += 1\n",
    "\n",
    "    hashes1 = list(cache1.keys())\n",
    "    hashes2 = list(cache2.keys())\n",
    "    emb1 = np.vstack(list(cache1.values()))\n",
    "    emb2 = np.vstack(list(cache2.values()))\n",
    "    \n",
    "    distance = DataArray(cdist(emb1, emb2, metric='cosine'),\n",
    "                         dims=('file1', 'file2'),\n",
    "                         coords=(hashes1, hashes2))\n",
    "    \n",
    "    return cache1, cache2, index1, index2, distance \n",
    "\n",
    "# A function to calculate the EER on a subset of VoxCeleb1_X\n",
    "def run_experiment(index1, index2, distance, trialFile):\n",
    "    \n",
    "    y_pred, y_true = [], []\n",
    "    with open(trialFile, 'r') as tf:\n",
    "        for trialPair in tf:\n",
    "            file1, file2 = trialPair.split(' ')[1], trialPair.split(' ')[2]\n",
    "            key1 = file1.split('/')[0] + '-' + file1.split('/')[1] + \\\n",
    "                    '-' + file1.split('/')[-1].split('.wav')[0]\n",
    "            key2 = file2.split('/')[0] + '-' + file2.split('/')[1] + \\\n",
    "                    '-' + file2.split('/')[-1].split('.wav')[0]\n",
    "            y_pred.append(distance.data[index1[key1], index2[key2]])\n",
    "            y_true.append(int(trialPair.split(' ')[0]))\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    eer, ci_lower, ci_upper, _ = feerci(-y_pred[y_true == 0],\n",
    "                                        -y_pred[y_true == 1],\n",
    "                                        is_sorted=False)\n",
    "    return {\n",
    "        'eer': eer,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153516it [00:05, 26120.59it/s]\n"
     ]
    }
   ],
   "source": [
    "exp_folder = \"/home/ge/kaldi/egs/voxceleb/pytorch/exp/ecapa-tdnn-tf-new\"\n",
    "trial_file = '/storage/ge/voxceleb/test/wav/veri_test2.txt'\n",
    "scp_file = os.path.join(exp_folder, 'kaldi_ark', 'vox1', 'xvector_a_vox1.scp')\n",
    "cohortEmbeds, testEmbeds = collect_vec(scp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache1, cache2, index1, index2, distances = get_trial_vec(testEmbeds, trial_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eer': 0.0174458809196949, 'ci_lower': 0.016435295343399048, 'ci_upper': 0.01882878504693508, 'y_true': array([1, 0, 1, ..., 0, 1, 0]), 'y_pred': array([0.45178772, 0.94382471, 0.43102617, ..., 0.99288202, 0.5142736 ,\n",
      "       1.02745815])}\n"
     ]
    }
   ],
   "source": [
    "eer_dict = run_experiment(index1, index2, distances, trial_file)\n",
    "print(eer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get cohort embeddings from VoxCeleb1\n",
    "cohortSpkrs = list(cohortEmbeds.keys())\n",
    "cohort = np.vstack([np.mean(np.vstack(cohortEmbeds[speaker]), axis=0, keepdims=True) \n",
    "                    for speaker in cohortSpkrs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities on train and val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hashes1 = list(cache1.keys())\n",
    "hashes2 = list(cache2.keys())\n",
    "\n",
    "emb1 = np.vstack(list(cache1.values()))\n",
    "emb2 = np.vstack(list(cache2.values()))\n",
    "# Calculate the distances between each trial embedding (file1 and file2) and the cohort\n",
    "distance1 = DataArray(\n",
    "    cdist(emb1, cohort, metric='cosine'),\n",
    "    dims=('file1', 'cohort'),\n",
    "    coords=(hashes1, cohortSpkrs))\n",
    "\n",
    "distance2 = DataArray(\n",
    "    cdist(emb2, cohort, metric='cosine'),\n",
    "    dims=('file2', 'cohort'),\n",
    "    coords=(hashes2, cohortSpkrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize scores w.r.t the N most similar cohort embeddingsÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our N\n",
    "COHORT_SIZE = 400\n",
    "\n",
    "# Calculate mean and std of N most similar cohort embeddings for file1\n",
    "data1 = np.partition(distance1.data, COHORT_SIZE)[:, :COHORT_SIZE]\n",
    "mz = np.mean(data1, axis=1) \n",
    "sz = np.std(data1, axis=1)\n",
    "mz = DataArray(mz, dims=('file1',), coords=(hashes1,))\n",
    "sz = DataArray(sz, dims=('file1',), coords=(hashes1,))\n",
    "\n",
    "# Calculate mean and std of N most similar cohort embeddings for file2\n",
    "data2 = np.partition(distance2.data, COHORT_SIZE)[:, :COHORT_SIZE]\n",
    "mt = np.mean(data2, axis=1) \n",
    "st = np.std(data2, axis=1)\n",
    "mt = DataArray(mt, dims=('file2',), coords=(hashes2,))\n",
    "st = DataArray(st, dims=('file2',), coords=(hashes2,))\n",
    "\n",
    "# Normalize\n",
    "distance_z = (distances - mz) / sz\n",
    "distance_t = (distances - mt) / st\n",
    "distance_s = 0.5 * (distance_z + distance_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eer': 0.016322851181030273, 'ci_lower': 0.015105579048395157, 'ci_upper': 0.018077433109283447, 'y_true': array([1, 0, 1, ..., 0, 1, 0]), 'y_pred': array([-6.74822005,  0.88052302, -6.67566938, ...,  1.96855125,\n",
      "       -4.77046657,  2.83883655])}\n"
     ]
    }
   ],
   "source": [
    "snorm_eer_dict = run_experiment(index1, index2, distance_s, trial_file)\n",
    "print(snorm_eer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
